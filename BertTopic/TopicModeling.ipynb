{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# BERTopic + llama-cpp-python\n",
    "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python\n",
    "!pip install bertopic datasets\n",
    "\n",
    "# DataMapPlot\n",
    "!git clone https://github.com/TutteInstitute/datamapplot.git\n",
    "!pip install datamapplot/.\n",
    "\n",
    "# GPU-accelerated HDBSCAN + UMAP\n",
    "!pip install cudf-cu12 dask-cudf-cu12 --extra-index-url=https://pypi.nvidia.com\n",
    "!pip install cuml-cu12 --extra-index-url=https://pypi.nvidia.com\n",
    "!pip install cugraph-cu12 --extra-index-url=https://pypi.nvidia.com\n",
    "!pip install cupy-cuda12x -f https://pip.cupy.dev/aarch64\n",
    "\n",
    "!wget https://huggingface.co/TheBloke/OpenHermes-2.5-Mistral-7B-GGUF/resolve/main/openhermes-2.5-mistral-7b.Q4_K_M.gguf\n",
    "# !wget https://huggingface.co/TheBloke/dolphin-2.7-mixtral-8x7b-GGUF/resolve/main/dolphin-2.7-mixtral-8x7b.Q3_K_M.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install boto3==1.18.62\n",
    "%pip install google-api-python-client==2.112.0 \n",
    "%pip install langchain==0.1.4 \n",
    "%pip install python-dotenv==1.0.0\n",
    "%pip install tiktoken==0.5.2\n",
    "%pip install streamlit==1.36.0\n",
    "%pip install tqdm==4.66.1\n",
    "%pip install youtube-transcript-api==0.6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import boto3\n",
    "from dateutil.tz import tzutc\n",
    "from google.colab import userdata\n",
    "\n",
    "\n",
    "AWS_ACCESS_KEY_ID = userdata.get('AWS_ACCESS_KEY_ID')\n",
    "AWS_ACCESS_SECRET_KEY = userdata.get('AWS_SECRET_ACCESS_KEY')\n",
    "S3_BUCKET_NAME = userdata.get('S3_BUCKET_NAME')\n",
    "AWS_REGION = userdata.get('AWS_REGION')\n",
    "\n",
    "\n",
    "class S3Utils:\n",
    "    def __init__(self):\n",
    "        self.client = boto3.client(\n",
    "            \"s3\",\n",
    "            aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "            aws_secret_access_key=AWS_ACCESS_SECRET_KEY,\n",
    "        )\n",
    "\n",
    "    def get_s3_objects(self, file_path):\n",
    "        \"\"\"\n",
    "        Retrieves an object from an S3 bucket.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The path of the file in the S3 bucket.\n",
    "\n",
    "        Returns:\n",
    "            dict: The response object containing the retrieved object.\n",
    "\n",
    "        \"\"\"\n",
    "        res = self.client.get_object(Bucket=S3_BUCKET_NAME, Key=file_path)\n",
    "        return res\n",
    "\n",
    "    def download_s3_file(self, local_file_name, s3_object_key):\n",
    "        meta_data = self.client.head_object(Bucket=S3_BUCKET_NAME, Key=s3_object_key)\n",
    "        total_length = int(meta_data.get(\"ContentLength\", 0))\n",
    "        downloaded = 0\n",
    "\n",
    "        def progress(chunk):\n",
    "            nonlocal downloaded\n",
    "            downloaded += chunk\n",
    "            done = int(50 * downloaded / total_length)\n",
    "            sys.stdout.write(\"\\r[%s%s]\" % (\"=\" * done, \" \" * (50 - done)))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        with open(local_file_name, \"wb\") as f:\n",
    "            self.client.download_fileobj(\n",
    "                S3_BUCKET_NAME, s3_object_key, f, Callback=progress\n",
    "            )\n",
    "\n",
    "        print(\n",
    "            f\"\\nDownloaded {local_file_name} from S3 bucket {S3_BUCKET_NAME} Key {s3_object_key}\"\n",
    "        )\n",
    "\n",
    "    def upload_file_to_s3(self, file_path, uploadto):\n",
    "        \"\"\"\n",
    "        Uploads a file to Amazon S3.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The path of the file to be uploaded.\n",
    "            uploadto (str): The destination folder in S3 where the file will be uploaded.\n",
    "\n",
    "        Returns:\n",
    "            str: The downloadable URL of the uploaded file.\n",
    "        \"\"\"\n",
    "        self.client.upload_file(\n",
    "            Bucket=S3_BUCKET_NAME,\n",
    "            Filename=file_path,\n",
    "            Key=uploadto,\n",
    "        )\n",
    "\n",
    "    def upload_folder_to_s3(self, local_folder, s3_folder):\n",
    "        \"\"\"\n",
    "        Uploads all files in a local folder to a specified S3 folder.\n",
    "\n",
    "        Args:\n",
    "            local_folder (str): The path to the local folder containing the files to upload.\n",
    "            s3_folder (str): The path to the S3 folder where the files will be uploaded.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        for root, dirs, files in os.walk(local_folder):\n",
    "            for file in files:\n",
    "                local_path = os.path.join(root, file)\n",
    "                relative_path = os.path.relpath(local_path, local_folder)\n",
    "                s3_path = os.path.join(s3_folder, relative_path)\n",
    "                self.client.upload_file(local_path, S3_BUCKET_NAME, s3_path)\n",
    "\n",
    "    def upload_file_text_to_s3(self, text, uploadto):\n",
    "        \"\"\"\n",
    "        Uploads a text file to an S3 bucket.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text content to be uploaded.\n",
    "            uploadto (str): The key or path where the file will be uploaded in the S3 bucket.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.client.put_object(Bucket=S3_BUCKET_NAME, Key=uploadto, Body=text)\n",
    "\n",
    "    def download_folder_from_s3(self, s3_folder, local_folder):\n",
    "        # Get all objects in the S3 folder\n",
    "        response = self.client.list_objects_v2(Bucket=S3_BUCKET_NAME, Prefix=s3_folder)\n",
    "\n",
    "        if response.get(\"Contents\"):\n",
    "            objects = response[\"Contents\"]\n",
    "\n",
    "            # Download each object\n",
    "            for obj in objects:\n",
    "                s3_path = obj[\"Key\"]\n",
    "                local_path = os.path.join(\n",
    "                    local_folder, os.path.relpath(s3_path, s3_folder)\n",
    "                )\n",
    "                local_dir = os.path.dirname(local_path)\n",
    "\n",
    "                # Create local directories if they don't exist\n",
    "                if not os.path.exists(local_dir):\n",
    "                    os.makedirs(local_dir)\n",
    "\n",
    "                # Download the object\n",
    "                self.client.download_file(S3_BUCKET_NAME, s3_path, local_path)\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def filterd_list_md5_checksum(self, directory_name):\n",
    "        \"\"\"\n",
    "        Filters the list of objects in the specified S3 bucket directory based on the last modified\n",
    "        timestamp and file extensions.\n",
    "\n",
    "        Args:\n",
    "            directory_name (str): The name of the S3 bucket directory to filter.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of MD5 checksums for the filtered S3 objects.\n",
    "        \"\"\"\n",
    "        condition_timestamp = datetime.now(tz=tzutc()) - timedelta(minutes=5)\n",
    "        paginator = self.client.get_paginator(\"list_objects_v2\")\n",
    "        s3_filtered_list = []\n",
    "\n",
    "        for page in paginator.paginate(Bucket=S3_BUCKET_NAME, Prefix=directory_name):\n",
    "            if \"Contents\" in page:\n",
    "                for obj in page[\"Contents\"]:\n",
    "                    if obj[\"LastModified\"] > condition_timestamp and (\n",
    "                        obj[\"Key\"].lower().endswith(\".pdf\")\n",
    "                        or obj[\"Key\"].lower().endswith(\".txt\")\n",
    "                    ):\n",
    "                        s3_filtered_list.append(obj)\n",
    "\n",
    "        object_md5_checksums = [obj[\"ETag\"].strip('\"') for obj in s3_filtered_list]\n",
    "\n",
    "        return object_md5_checksums\n",
    "\n",
    "\n",
    "S3 = S3Utils()\n",
    "\n",
    "def load_docs_from_jsonl(local_path):\n",
    "    documents = []\n",
    "    with open(local_path, \"r\") as jsonl_file:\n",
    "        for line in jsonl_file:\n",
    "            data = json.loads(line)\n",
    "            obj = Document(**data)\n",
    "            documents.append(obj)\n",
    "    return documents\n",
    "\n",
    "\n",
    "def dowload_document(uuid, local_file_path):\n",
    "    S3.download_s3_file(\n",
    "        local_file_path, f\"{uuid}/{os.path.basename(local_file_path)}\"\n",
    "    )\n",
    "    documents = load_docs_from_jsonl(local_file_path)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = dowload_document('8adf1145-b055-4710-b26a-8f52fac88def', 'Ludwig_2a1ddefc8bcb4653951633c58a01b663_documents.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for doc in docs:\n",
    "  texts.append(doc.page_content)\n",
    "\n",
    "docs = texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# Use llama.cpp to load in a Quantized LLM\n",
    "llm = Llama(model_path=\"openhermes-2.5-mistral-7b.Q4_K_M.gguf\", n_gpu_layers=-1, n_ctx=4096, stop=[\"Q:\", \"\\n\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic.representation import KeyBERTInspired, LlamaCPP\n",
    "\n",
    "prompt = \"\"\" Q:\n",
    "I have a topic that contains the following documents:\n",
    "[DOCUMENTS]\n",
    "\n",
    "The topic is described by the following keywords: '[KEYWORDS]'.\n",
    "\n",
    "Based on the above information, can you give a short label of the topic of at most 5 words?\n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "representation_model = {\n",
    "    \"KeyBERT\": KeyBERTInspired(),\n",
    "    \"LLM\": LlamaCPP(llm, prompt=prompt),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from cuml.manifold import UMAP\n",
    "from cuml.cluster import HDBSCAN\n",
    "# from umap import UMAP\n",
    "# from hdbscan import HDBSCAN\n",
    "\n",
    "# Pre-calculate embeddings\n",
    "embedding_model = SentenceTransformer(\"BAAI/bge-small-en\")\n",
    "embeddings = embedding_model.encode(docs, show_progress_bar=True)\n",
    "\n",
    "# Pre-reduce embeddings for visualization purposes\n",
    "reduced_embeddings = UMAP(n_neighbors=15, n_components=2, min_dist=0.0, metric='cosine', random_state=42).fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sub-models\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=400, metric='euclidean', cluster_selection_method='eom', prediction_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "topic_model = BERTopic(\n",
    "\n",
    "  # Sub-models\n",
    "  embedding_model=embedding_model,\n",
    "  umap_model=umap_model,\n",
    "  hdbscan_model=hdbscan_model,\n",
    "  representation_model=representation_model,\n",
    "\n",
    "  # Hyperparameters\n",
    "  top_n_words=10,\n",
    "  verbose=True\n",
    ")\n",
    "\n",
    "# Train model\n",
    "topics, probs = topic_model.fit_transform(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datamapplot\n",
    "import re\n",
    "\n",
    "# Create a label for each document\n",
    "llm_labels = [re.sub(r'\\W+', ' ', label[0][0].split(\"\\n\")[0].replace('\"', '')) for label in topic_model.get_topics(full=True)[\"LLM\"].values()]\n",
    "llm_labels = [label if label else \"Unlabelled\" for label in llm_labels]\n",
    "all_labels = [llm_labels[topic+topic_model._outliers] if topic != -1 else \"Unlabelled\" for topic in topics]\n",
    "\n",
    "# Run the visualization\n",
    "datamapplot.create_plot(\n",
    "    reduced_embeddings,\n",
    "    all_labels,\n",
    "    label_font_size=11,\n",
    "    title=\"ArXiv - BERTopic\",\n",
    "    sub_title=\"Topics labeled with `openhermes-2.5-mistral-7b`\",\n",
    "    label_wrap_width=20,\n",
    "    use_medoids=True,\n",
    "    logo_width=0.16\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
